# Kafka Integration

## Implementation Overview

### 1. Docker Compose Setup (`docker-compose.yml`)

- Zookeeper service
- Kafka broker (with proper configuration)
- vLLM server
- Health checks and dependencies

### 2. Kafka LLM Processor (`src/kafka_llm_processor.py`)

Production-ready consumer that:
- Consumes documents from `financial-documents` topic
- Processes through vLLM with error handling and retries
- Publishes results to `llm-results` topic
- Includes logging, metrics, graceful shutdown
- Handles Kafka errors and LLM timeouts
- Supports configuration via environment variables

### 3. Test Producer (`src/test_producer.py`)

Production-ready producer that:
- Sends sample financial documents to Kafka
- Includes 6 sample documents (earnings reports, SEC filings, news)
- Proper error handling and retries
- Command-line interface with options

### 4. Test Consumer (`src/test_consumer.py`)

Consumer to verify results:
- Reads from `llm-results` topic
- Pretty-prints formatted results
- Shows processing time, tokens used, status

### 5. Helper Scripts

- `scripts/run_pipeline.ps1` - Start all services
- `scripts/test_pipeline.py` - Verify services are running

## Quick Start

### Step 1: Start Infrastructure

```bash
docker-compose up -d
```

Wait ~30 seconds for services to start.

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 3: Start Processor (Terminal 1)

```bash
python src/kafka_llm_processor.py
```

### Step 4: Send Test Data (Terminal 2)

```bash
python src/test_producer.py --count 3
```

### Step 5: View Results (Terminal 3)

```bash
python src/test_consumer.py
```

## Architecture

```
Kafka Producer → financial-documents topic → Kafka Consumer → vLLM → llm-results topic → Result Consumer
```

## Configuration

Environment variables (with defaults):
- `KAFKA_BROKERS`: `localhost:9092`
- `INPUT_TOPIC`: `financial-documents`
- `OUTPUT_TOPIC`: `llm-results`
- `LLM_URL`: `http://localhost:8000`
- `MODEL_NAME`: `mistralai/Mistral-7B-Instruct-v0.2`
- `LLM_TIMEOUT`: `30`

## Production Features

- Error handling and retries
- Logging with file and console output
- Graceful shutdown handling
- Request/response tracking
- Processing time metrics
- Token usage tracking
- Compression (gzip)
- Batch processing
- Health checks

## Testing

Run verification:

```bash
python scripts/test_pipeline.py
```

This checks:
- vLLM server health
- Kafka connectivity
- All services running

## Next Steps

1. Add audit trail logging
2. Add drift detection
3. Deploy to Kubernetes
